{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5968fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers.optimization import AdamW, get_constant_schedule_with_warmup\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, StochasticWeightAveraging\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AutoFeatureExtractor, HubertForSequenceClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809505a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa4266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAudios(df):\n",
    "    audios = []\n",
    "    for idx, row in tqdm(df.iterrows(),total=len(df)):\n",
    "        audio,_ = librosa.load(row['path'],sr=SAMPLING_RATE)\n",
    "        audios.append(audio)\n",
    "    return audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a88e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, audio, audio_feature_extractor, label=None):\n",
    "        if label is None:\n",
    "            label = [0] * len(audio)\n",
    "        self.label = np.array(label).astype(np.int64)\n",
    "        self.audio = audio\n",
    "        self.audio_feature_extractor = audio_feature_extractor\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label[idx]\n",
    "        audio = self.audio[idx]\n",
    "        audio_feature = audio_feature_extractor(raw_speech = audio, return_tensors='np', sampling_rate=SAMPLING_RATE)\n",
    "        audio_values, audio_attn_mask = audio_feature['input_values'][0], audio_feature['attention_mask'][0]\n",
    "\n",
    "        item = {\n",
    "            'label':label,\n",
    "            'audio_values':audio_values,\n",
    "            'audio_attn_mask':audio_attn_mask,\n",
    "        }\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef89dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    batch_labels = []\n",
    "    batch_audio_values = []\n",
    "    batch_audio_attn_masks = []\n",
    "\n",
    "    for sample in samples:\n",
    "        batch_labels.append(sample['label'])\n",
    "        batch_audio_values.append(torch.tensor(sample['audio_values']))\n",
    "        batch_audio_attn_masks.append(torch.tensor(sample['audio_attn_mask']))\n",
    "\n",
    "    batch_labels = torch.tensor(batch_labels)\n",
    "    batch_audio_values = pad_sequence(batch_audio_values, batch_first=True)\n",
    "    batch_audio_attn_masks = pad_sequence(batch_audio_attn_masks, batch_first=True)\n",
    "\n",
    "    batch = {\n",
    "        'label': batch_labels,\n",
    "        'audio_values': batch_audio_values,\n",
    "        'audio_attn_mask': batch_audio_attn_masks,\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62e3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLitModel(pl.LightningModule):\n",
    "    def __init__(self, audio_model_name, num_labels, n_layers=1, projector=True, classifier=True, dropout=0.07, lr_decay=1):\n",
    "        super(MyLitModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(audio_model_name)\n",
    "        self.config.activation_dropout=dropout\n",
    "        self.config.attention_dropout=dropout\n",
    "        self.config.final_dropout=dropout\n",
    "        self.config.hidden_dropout=dropout\n",
    "        self.config.hidden_dropout_prob=dropout\n",
    "        self.audio_model = HubertForSequenceClassification.from_pretrained(audio_model_name, config=self.config)\n",
    "        self.lr_decay = lr_decay\n",
    "        self._do_reinit(n_layers, projector, classifier)\n",
    "\n",
    "    def forward(self, audio_values, audio_attn_mask):\n",
    "        logits = self.audio_model(input_values=audio_values, attention_mask=audio_attn_mask).logits\n",
    "        logits = torch.stack([\n",
    "            logits[:,0]+logits[:,7],\n",
    "            logits[:,2]+logits[:,9],\n",
    "            logits[:,5]+logits[:,12],\n",
    "            logits[:,1]+logits[:,8],\n",
    "            logits[:,4]+logits[:,11],\n",
    "            logits[:,3]+logits[:,10]]\n",
    "        , dim=-1)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        audio_values = batch['audio_values']\n",
    "        audio_attn_mask = batch['audio_attn_mask']\n",
    "        labels = batch['label']\n",
    "\n",
    "        logits = self(audio_values, audio_attn_mask)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, labels)\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        audio_values = batch['audio_values']\n",
    "        audio_attn_mask = batch['audio_attn_mask']\n",
    "        labels = batch['label']\n",
    "\n",
    "        logits = self(audio_values, audio_attn_mask)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, labels)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        audio_values = batch['audio_values']\n",
    "        audio_attn_mask = batch['audio_attn_mask']\n",
    "\n",
    "        logits = self(audio_values, audio_attn_mask)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = 1e-5\n",
    "        layer_decay = self.lr_decay\n",
    "        weight_decay = 0.01\n",
    "        llrd_params = self._get_llrd_params(lr=lr, layer_decay=layer_decay, weight_decay=weight_decay)\n",
    "        optimizer = AdamW(llrd_params)\n",
    "        return optimizer\n",
    "\n",
    "    def _get_llrd_params(self, lr, layer_decay, weight_decay):\n",
    "        n_layers = self.audio_model.config.num_hidden_layers\n",
    "        llrd_params = []\n",
    "        for name, value in list(self.named_parameters()):\n",
    "            if ('bias' in name) or ('layer_norm' in name):\n",
    "                llrd_params.append({\"params\": value, \"lr\": lr, \"weight_decay\": 0.0})\n",
    "            elif ('emb' in name) or ('feature' in name) : \n",
    "                llrd_params.append({\"params\": value, \"lr\": lr * (layer_decay**(n_layers+1)), \"weight_decay\": weight_decay})\n",
    "            elif 'encoder.layer' in name:\n",
    "                for n_layer in range(n_layers):\n",
    "                    if f'encoder.layer.{n_layer}' in name:\n",
    "                        llrd_params.append({\"params\": value, \"lr\": lr * (layer_decay**(n_layer+1)), \"weight_decay\": weight_decay})\n",
    "            else:\n",
    "                llrd_params.append({\"params\": value, \"lr\": lr , \"weight_decay\": weight_decay})\n",
    "        return llrd_params\n",
    "    \n",
    "    def _do_reinit(self, n_layers=0, projector=True, classifier=True):\n",
    "        if projector:\n",
    "            self.audio_model.projector.apply(self._init_weight_and_bias)\n",
    "        if classifier:\n",
    "            self.audio_model.classifier.apply(self._init_weight_and_bias)\n",
    "        \n",
    "        for n in range(n_layers):\n",
    "            self.audio_model.hubert.encoder.layers[-(n+1)].apply(self._init_weight_and_bias)\n",
    "            \n",
    "    def _init_weight_and_bias(self, module):                        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.audio_model.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dba62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'\n",
    "PREPROC_DIR = './preproc'\n",
    "SUBMISSION_DIR = './submission'\n",
    "MODEL_DIR = './model'\n",
    "SAMPLING_RATE = 16000\n",
    "SEED=0\n",
    "N_FOLD=20\n",
    "BATCH_SIZE=8\n",
    "NUM_LABELS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512accaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model_name = 'Rajaram1996/Hubert_emotion'\n",
    "audio_feature_extractor = AutoFeatureExtractor.from_pretrained(audio_model_name)\n",
    "audio_feature_extractor.return_attention_mask=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43034f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preproc = pd.read_csv(f\"{PREPROC_DIR}/train_df_preproc.csv\")\n",
    "test_preproc = pd.read_csv(f\"{PREPROC_DIR}/test_df_preproc.csv\")\n",
    "train_preproc['path'] = train_preproc['path'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
    "test_preproc['path'] = test_preproc['path'].apply(lambda x: os.path.join(DATA_DIR, x))\n",
    "train_audios = getAudios(train_preproc)\n",
    "test_audios = getAudios(test_preproc)\n",
    "train_label = train_preproc['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_FOLD,shuffle=True,random_state=SEED)\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(skf.split(train_label, train_label)):\n",
    "    train_fold_audios = [train_audios[train_index] for train_index in train_indices]\n",
    "    val_fold_audios= [train_audios[val_index] for val_index in val_indices]\n",
    "\n",
    "    train_fold_label = train_label[train_indices]\n",
    "    val_fold_label = train_label[val_indices]\n",
    "    train_fold_ds = MyDataset(train_fold_audios, audio_feature_extractor, train_fold_label)\n",
    "    val_fold_ds = MyDataset(val_fold_audios, audio_feature_extractor, val_fold_label)\n",
    "    train_fold_dl = DataLoader(train_fold_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    val_fold_dl = DataLoader(val_fold_ds, batch_size=BATCH_SIZE*2, collate_fn=collate_fn)\n",
    "\n",
    "    checkpoint_acc_callback = ModelCheckpoint(\n",
    "        monitor='val_acc',\n",
    "        dirpath=MODEL_DIR,\n",
    "        filename=f'{fold_idx=}'+'_{epoch:02d}-{val_acc:.4f}-{train_acc:.4f}',\n",
    "        save_top_k=1,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    my_lit_model = MyLitModel(\n",
    "        audio_model_name=audio_model_name,\n",
    "        num_labels=NUM_LABELS,\n",
    "        n_layers=1, projector=True, classifier=True, dropout=0.07, lr_decay=0.8\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='cuda', \n",
    "        max_epochs=30,\n",
    "        precision=16,\n",
    "        val_check_interval=0.1,\n",
    "        callbacks=[checkpoint_acc_callback],\n",
    "    )\n",
    "\n",
    "    trainer.fit(my_lit_model, train_fold_dl, val_fold_dl)\n",
    "\n",
    "    del my_lit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b2f3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = MyDataset(test_audios, audio_feature_extractor)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE*2, collate_fn=collate_fn)\n",
    "pretrained_models = list(map(lambda x: os.path.join(MODEL_DIR,x),os.listdir(MODEL_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='cuda', \n",
    "    precision=16,\n",
    ")\n",
    "for pretrained_model_path in pretrained_models:\n",
    "    pretrained_model = MyLitModel.load_from_checkpoint(\n",
    "        pretrained_model_path,\n",
    "        audio_model_name=audio_model_name,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )\n",
    "    test_pred = trainer.predict(pretrained_model, test_dl)\n",
    "    test_pred = torch.cat(test_pred).detach().cpu().numpy()\n",
    "    test_preds.append(test_pred)\n",
    "    del pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a4cda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(os.path.join(DATA_DIR,'sample_submission.csv'))\n",
    "submission_df['label'] = list(map(np.argmax,(map(np.bincount,np.array(test_preds).T))))\n",
    "submission_df.to_csv(os.path.join(SUBMISSION_DIR,'20_fold.csv'),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
